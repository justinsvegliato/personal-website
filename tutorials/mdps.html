<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="description" content="Justin Svegliato, a graduate student at UMass Amherst passionate about automated reasoning, reinforcement learning, and neural networks.">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">

    <title>Justin Svegliato</title>

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:regular,bold,italic,thin,light,bolditalic,black,medium&amp;lang=en">
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://code.getmdl.io/1.1.3/material.min.css">
    <link rel="stylesheet" href="../css/style.css">
    <link rel="stylesheet" href="../css/tutorials.css">

    <script defer src="https://code.getmdl.io/1.1.3/material.min.js"></script>
    <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.4/jquery.min.js"></script>
    <script defer src="../js/site.js"></script>
    <script defer src="../js/mdps.js"></script>
</head>

<body>
<div class="mdl-layout mdl-js-layout mdl-layout--fixed-drawer">
    <div class="mdl-layout__drawer">
        <span class="mdl-layout-title"><a id="back-navigation-link" href="../index.html"><i id="back-icon" class="material-icons">keyboard_arrow_left</i> BACK</a></span>

        <nav class="mdl-navigation">
            <a class="mdl-navigation__link active" href="#introduction">Introduction</a>
            <a class="mdl-navigation__link" href="#definition">Definition</a>
            <a class="mdl-navigation__link" href="#policies">Policies</a>
        </nav>
    </div>

    <div class="mdl-layout__content">
        <div id="introduction" class="section">
            <div class="section-body">
                <h2>Markov Decision Processes</h2>

                <h3>Introduction</h3>
                <p>Suppose we're a treasure hunter (like Indiana Jones or something) searching for a famous treasure chest hidden somewhere in the world. For the sake of simplicity, let's just assume our world isn't the earth but instead a 5x5 grid. In order to search for the treasure, we can move from one cell to another by going north, east, south, or west. Moreover, while we don't know the world at all, we at least know that one of the cells contains the treasure. So, if we could navigate the world successfully and find the treasure, we would be immeasurably rich like Tywin Lannister and hopefully just as awesome. Unfortunately, there's a catch. While most of the cells in the world are empty, there are a few that have traps in them like a giant cat monster or a pit of spikes. If we were to accidentally move into any of those cells, we'd probably die a painful death.</p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/grid-world.png" /></p>
                <p>Since we want to hit it rich and not die, we need to come up with a plan that leads to the treasure but avoids the traps. In our case, a plan is just the series of moves that we should take (e.g., north, east, south, or west) from our current location to the goal state (e.g., the treasure). After we complete every action in our plan, we'll end up at the treasure (which means we weren't a giant cat monster's supper). Sounds pretty easy, right? Just (1) come up with a plan that goes from our current location to the treasure, then (2) follow that plan by doing the suggested actions, and finally (3) reap the reward of infinite wealth!</p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/optimal-plan.png" /></p>
                <p>Sadly, it's not that easy. While it might be trivial in this simple problem, it's a lot harder in the real world to come up with a plan. In fact, if we had a harder problem, we'd need to represent it as mathematical model in order to make a plan. As a result, before we can deposit a nice, fat check into our bank accounts, we need to convert our treasure-hunting scenario into a mathematical model that we can solve using one of many different algorithms.</p>
                <p>For now, this is the big question we need to figure out: <strong>how do we come up with a mathematical model that represents our scenario to snag the treasure without getting impaled by spikes or eaten by a giant cat monster?</strong>
            </div>
        </div>

        <hr class="topic-separator" />

        <div id="definition" class="section">
            <div class="section-body">
                <h3>Definition</h3>
                <p>To model any scenario like this, we can use a <strong>Markov decision process</strong> (or an <strong>MDP</strong>). An MDP is just a formal representation of the type of problem we just saw. Generally, it's composed of four parts:</p>

                <h5>States</h5>
                <p>The set <span class="inline-equation">S</span> contains every state of the world. Each cell in our treasure-hunting world (which, as a reminder, is a 5x5 grid) corresponds to a state. This means we have a set of twenty-five states:</p>
                <div class="equation">S = {s0, s1, ..., s24}</div>
                <p>For your convenience, here's an image that labels every state of the world:</p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/grid-world-states.png" /></p>

                <h5>Actions</h5>
                <p>The set <span class="inline-equation">A</span> contains every action that the <strong>agent</strong> can take in the world. By the way, if you don't know what an agent is, it's just the actor in the world. It could be you, a robot, a car, or anything else that follows a plan. In any case, we have a set of five actions in our example:</p>
                <div class="equation">A = {North, East, South, West, Stay}</div>
                <p>Why do we have the <span class="inline-equation">Stay</span> action? This lets us sit around at the treasure once we've found it: we wouldn't want to move away from it once we've discovered the treasure after all of our hard work.</p>
                <p>Keep in mind that each state has certain constraints that limit the actions available to the agent. For instance, if we were in the corner of the world, say, the top left corner, we couldn't move <span class="inline-equation">North</span> or <span class="inline-equation">West</span> since we're at the boundary of the world. We typically denote the actions available in a given state <span class="inline-equation">s</span> as <span class="inline-equation">A(s)</span>. So, if we're in the top right corner of the world, <span class="inline-equation">s20</span>, these would be the only actions available to the agent:</p>
                <div class="equation">A(s20) = {East, South, Stay}</div>
                <p>This visualization should probably help:</p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/grid-world-actions.png" /></p>

                <h5>Transitions</h5>
                <p>The function <span class="inline-equation">T(s, a)</span> is the <strong>state-transition function</strong>. It returns a probability distribution over every possible state of the world <span class="inline-equation">S</span> if the agent were to perform action <span class="inline-equation">a</span> in state <span class="inline-equation">s</span>. For example, if we were in the bottom left corner of the world, say, <span class="inline-equation">s0</span>, and we performed the action <span class="inline-equation">North</span>, the function <span class="inline-equation">T(s0, North)</span> would return the following probability distribution: <div class="equation">[s0: 0.0, s1: 0.0, ... , s5: 1.0, ..., s24: 0.0, s25: 0.0]</div></p>
                <p>This funny notation just assigns a probability of entering each state of the world if we performed action <span class="inline-equation">North</span> in state <span class="inline-equation">s0</span>. Since our world is <strong>deterministic</strong>, the state <span class="inline-equation">s5</span> has a probability of 1.0. Check this out to see what I mean:</p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/deterministic-grid-world.png" /></p>
                <p>In real world examples, however, the outcomes of actions are typically <strong>probabilistic</strong>. This means that an action could lead to different states, not just one like our example. In a more realistic MDP, we could perhaps associate a probability of slipping with every action. So, even if we performed the action <span class="inline-equation">North</span>, we could slip and accidentally end up in the wrong state. This happens a lot in robotics:</p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/probabilistic-grid-world.png" /></p>
                <p>For now, let's just assume we have a deterministic world to make things easy.</p>

                <h5>Rewards</h5>
                <p>The function <span class="inline-equation">R(s, a)</span> is the <strong>reward function</strong>. It returns the reward that the agent after performing action <span class="inline-equation">a</span> in state <span class="inline-equation">s</span>. In our world, let's assume that the treasure is in the top right corner of the world <span class="inline-equation">s23</span>. As a result, if we were in state <span class="inline-equation">s23</span>, which is the cell east of <span class="inline-equation">s24</span>, and we performed the action <span class="inline-equation">West</span>, the function <span class="inline-equation">R(s24, West)</span> would return <span class="inline-equation">1,000</span> since we just hit the treasure! Formally, we'd write this:</p>
                <div class="equation">R(s23, West) = 1000</div>
                <p>Performing any action in any empty cell would return a reward of <span class="inline-equation">-1</span> since moving repeatedly over empty cells should have some cost associated with it. If it didn't, we could do a few loops around the world before finding the treasure without penalty. We'd rather follow the shortest path to the treasure.</p>
                <p>Unfortunately, if we were to land in a state that has a giant cat monster or a pit of spikes, we would get a reward of <span class="inline-equation">-100,000</span>. This high, negative number represents our death. We probably want to avoid that at all costs unless we're a pretty sadistic treasure hunter.</p>
                <p>In short, here's how our world looks with rewards:</span></p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/grid-world-rewards.png" /></p>

                <p>These four components completely specify the problem that we outlined in the first section! But all this does is give us a way to model the problem mathematically as an MDP. In other words, while we now have our scenario represented as an MDP, we still haven't found the plan that we should follow to get the treasure. How do we go about doing that? That is, what actions should we take to get the treasure and avoid dying? There are a lot of fancy things that we can do to get the best plan from our MDP. However, before we talk about that, let's talk a little more about what plans are.</p>
            </div>
        </div>

        <hr class="topic-separator" />

        <div id="policies" class="section">
            <div class="section-body">
                <h3>Policies</h3>
                <p>In planning, we usually formulate a plan as a <strong>policy</strong>. By the way, this is only kind of true, but let's roll with it for now. Anyway, what is a policy? It's just a map that associates every state of the world with a recommended action:</p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/policy.png" /></p>
                <p>To be a little more formal, a policy <span class="inline-equation">&#960;</span> is a mapping between each state <span class="inline-equation">s</span> and an action <span class="inline-equation">a</span>:</p>
                <div class="equation">&#960;(s) = a</div>
                <p>That's great, Justin! I kinda get what a policy is now. Why do we need one? Why do we want to map actions to states? A policy is important because it specifies the action that the agent ought to take in every state of the world. For example, let's suppose the agent is in state <span class="inline-equation">s23</span>, which is directly east of the pot of goal. To figure out what action to take, we would take out our policy to see what action to perform in <span class="inline-equation">s23</span>. In an ideal world, our policy should tell us this (because it would make us move to the treasure):</p>
                <div class="equation">&#960;(s23) = West</div>
                <p>However, it's important to note that our policy doesn't have to associate each state with a good action! A policy is just something that links states to actions, and that's it. There are a ton of policies out there that we can have, and, among all of the garbage policies out there, we need to find the best one. If we followed a bad policy, we'd probably avoid the treasure and do something like this:</p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/bad-plan.png" /></p>
                <p>Notice how we walked right past the treasure. Yeah, that's not good. On the other hand, this is what we'd do if we followed the <strong>optimal policy</strong> (or the best policy we can have):</p>
                <p class="image-container"><img class="fixed-image" src="../img/tutorials/optimal-plan.png" /></p>
                <p>The big question in planning is now this: <strong>after we model the problem as an MDP, how do we come up with the best policy amidst all of the crappy polices out there?</strong> While there are many different approaches to this question, the next tutorial will discuss <strong>value iteration</strong>, one of the most basic (and naive) algorithms for finding the optimal policy. Stay tuned!</p>
            </div>
        </div>

        <hr class="topic-separator" />

        <footer class="mdl-mega-footer">
            <div class="mdl-mega-footer--middle-section">
                <div class="social-media-icons">
                    <a target="_blank" href="https://www.facebook.com/justin.svegliato">
                        <i class="fa fa-facebook social-media-icon" aria-hidden="true"></i>
                    </a>

                    <a target="_blank" href="https://github.com/justinsvegliato">
                        <i class="fa fa-github-alt social-media-icon" aria-hidden="true"></i>
                    </a>

                    <a target="_blank" href="https://www.linkedin.com/in/justinsvegliato">
                        <i class="fa fa-linkedin social-media-icon last" aria-hidden="true"></i>
                    </a>
                </div>

                <p id="copyright">Â© 2016 Justin Svegliato</p>
            </div>
        </footer>
    </div>
</div>
</body>
</html>